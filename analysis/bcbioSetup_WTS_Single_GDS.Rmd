---
title: "UMCCR bcbio single patient WTS config for data stored on GDS"
author: "Oliver Hofmann"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: readable
    toc: false
    toc_float: false
    code_folding: hide
---

```{r custom, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(tidyr)
library(rmarkdown)
library(dplyr)
library(DT)
library(readr)
library(forcats)
library(stringr)
library(janitor)
library(googledrive)
library(aws.s3)
library(aws.lambda)
library(here)
library(skimr)
library(purrr)
```

## Introduction

Code snippets to generate file summaries (for rsync'ing around) and bcbio sample summaries based on a [Google Spreadsheet](https://docs.google.com/spreadsheets/d/1DwvyfVrgr5TIcYtGVXZeIWWRbld-nuX-4o4z2kZHNWs/edit#gid=0), aka the dreadful LIMS stand-in. This version is meant for _single_ patient WTS samples only, i.e., samples are only batched for individual patients not across a whole project. This should be the default approach.

### 1. Setting up project information

This currently requires specifying a 'secondary use' field in the LIMS. Set the "secondary" analysis flag in the code snippet below to match the `secondary use` LIMS field used to highlight samples that need to be processed, regardless of processing status. Any sample with the matching `SECONDARY` entry in the `secondary analysis` column of the spreadsheet will be added as long as it is of the right `TYPE` (WTS for this script). 

The `googledrive` framework requires authentication with oAuth. This can be done interactive, but storing a token simplifies the process; see the [googlesheet authentication vignette](https://rawgit.com/jennybc/googlesheets/master/vignettes/managing-auth-tokens.html) for details. 

```{r project}
SECONDARY <- 'wts_58'

# Do not change
TYPE <- 'WTS'
PROJECT <- 'All'
```

### 2. Import data from Google spreadsheet

This step generates a data frame from the Google spreadsheet, unifying variable names and simplifying subject IDs for bcbio along the way. It replaces empty `results` cells (which indicate samples that still need processing) with a `-` to distinguish from true NAs and gets rid of whitespace in subject identifiers (although ideally there should not be any empty cells in the sheet in the first place). Otherwise standard data cleanup with `janitor`. We are also creating a timestamped backup of the Google Doc each time it is accessed, just in case.

```{r importData, message=FALSE}
# Create a backup copy each time we access the sample info
tm <- as.POSIXlt(Sys.time(), "UTC", "%Y-%m-%dT%H:%M:%S")
timestamp <- strftime(tm , "%Y-%m-%dT%H%M")
filename <- paste(timestamp, 'backup.csv', sep = '_')
backup_dir <- file.path(here::here('output', 'backup'))
dir.create(backup_dir, recursive = TRUE) # mkdir -p (warns if it already exists):

# Google Drive implementation
lims_key <- drive_find('^Google LIMS$', team_drive = 'LIMS')$id
drive_download(as_id(lims_key), path = file.path(backup_dir, filename), overwrite = TRUE)

# Import downloaded spreadsheet (sheet #1) as a tibble
samples_gs <- read_csv(file.path(backup_dir, filename), col_types = cols(.default = "c", Run = "d"))

# Tweak for analysis
#
# Currently mapping external_subject_id to subject_id (and same for sample id)
#
samples <- samples_gs %>%
  clean_names() %>%
  mutate(secondary_analysis = ifelse(is.na(secondary_analysis), '-', secondary_analysis)) %>%
  remove_empty(c('rows', 'cols')) %>%
  mutate(results = ifelse(is.na(results), '-', results)) %>%
  select(-matches('number_fastqs')) # Drop FASTQ count introduced with new version for now

# Add unique id to each row
samples <- samples %>%
  mutate(row_id = rownames(samples))
```

### 3. Find files ready for processing

Find all samples that either are part of the listed project and still need to be processed, or for which the `secondary flag` matches. Convert different sample naming conventions used throughout into a unified representation.

**Note:** This script relies on internal identifiers (a tuple of `subject_id`, `sample_id` and `library_id`). Where those are missing the script tries to replace them with the equivalent external identifier, or `missing` in the case of `library_id`. This works except for top-ups. If you need top-up samples to be merged make sure older samples have the appropriate internal identifiers set in the LIMS. 

```{r subsetSamples}
# Keep rows matching project and type requested; extract path to FASTQ
# and generate new file name for these
bcbio <- samples %>%
  mutate(project_name = if_else(project_owner == '-', 
                                project_name,
                                paste(project_owner, project_name, sep='-'))) %>%
  filter(type == TYPE &
           ((project_name == PROJECT & results == '-') | 
           (secondary_analysis == SECONDARY))) %>% 
  select(illumina_id, fastq, run, project_name, 
         subject_id, sample_id, library_id, 
         external_subject_id, external_sample_id, sample_name, 
         assay, phenotype, source, row_id) %>%
  mutate(subject_id = if_else(subject_id == '-', external_subject_id, subject_id)) %>%
  mutate(sample_id = if_else(sample_id == '-', external_sample_id, sample_id)) %>%
  mutate(library_id = if_else(library_id == '-', 'Missing', library_id)) %>%
  mutate(runname = illumina_id) %>%
#  separate(illumina_id, c(NA, 'machine', NA, NA), sep='_') 
  separate(illumina_id, c('tmpA', 'machine', 'tmpB', 'tmpC'), sep='_') %>%
  select(-tmpA, -tmpB, -tmpC)


# Add a 'target' name: what FASTQs are to be called after retrieval from S3
bcbio <- bcbio %>%
  mutate(targetname = paste(runname, subject_id, sample_id, library_id, sep = '_')) %>%
  mutate(targetname = str_replace_all(targetname, '-', '_')) 

# Also look up gds key
bcbio <- bcbio %>%
  mutate(description = paste(subject_id, sample_id, library_id, sep='_')) %>%
  mutate(description = str_replace(description, '_topup.', '')) %>%
  mutate(description = str_replace(description, '_topup', '')) %>%
  mutate(gds_key = paste(sample_id, library_id, sep='_'))
```

### 4. Generating bcbio csv templates

This step is relatively easy for WTS samples since we do not have to worry about batches, tumor/normal etc. Generate a file with pointers to the sample FASTQs and their preferred new names as well as a sample descriptor (`.csv`) for bcbio to use as part of it's templating approach. The resulting directory structure can be copied to Gadi, e.g.:

> `scp -r BATCHFILES USER@gadi-dm.nci.org.au:/g/data3/gx8/projects/PROJECT/`

On Gadi establish ICA credentials (e.g., via `ica login && ica projects enter production`) and run

> `find . -name *files.sh* -execdir sh {} \;`


```{r getGDSPath, eval=TRUE}
# Required information is stored in data portal; set up access credentials
profile <- 'prod'
aws.signature::use_credentials(profile = profile)

get_GDS_Path <- function(illumina_id) {
  
  gds <- paste('gds://umccr-fastq-data-prod/', illumina_id, sep='')
  results <- aws.lambda::invoke_function('data-portal-api-prod-fastq', 
                                         region='ap-southeast-2',
                                         payload=list(locations=list(gds)))
  
  results <- results %>%
    as_tibble() %>%
    mutate(samplename = names(fastq_map)) %>%
    unnest_wider(fastq_map) %>% 
    unnest_longer(fastq_list) %>%
    select(samplename, fastq_list) %>%
    rename(gds_key=samplename)

  return(results) 
}

# Get a list of Illumina RunIDs for the current samples
illumina_ids <- unique(bcbio$runname)

# Find all FASTQs for these runs
gds_info <- bind_rows(lapply(illumina_ids, get_GDS_Path))
```


```{r configbcbio}
write_subject <- function(subject, summary) {

    # Create a unique project name
  subject_name <- paste0(timestamp, '_', PROJECT, '_', TYPE, '_', subject)

  # Set up the project directory
  dir.create(here::here('output', subject_name, 'data'), 
             showWarnings = FALSE, recursive = TRUE)
     
  # Subset to one individual
  summary <- summary %>%
    filter(individual_id == subject)

  # Find samples in GDS summary
  gds_subset <- gds_info %>%
    filter(gds_key %in% summary$gds_key)
  
  # Find corresponding entries in the Google-Lims
  bcbio_subset <- bcbio %>%
     filter(row_id %in% summary$row_id)

  # Create a file list ready for linking
  gds_link <- bcbio_subset %>%
    right_join(gds_subset, by='gds_key') %>%
    rename(from = fastq_list) %>%
    mutate(to=paste(targetname, 
                     str_replace(basename(from), gds_key, ''),
                     sep='')) %>%
    select(from, to, row_id)
  
  # PBS templating
  file_handle <-  file(here::here('output', subject_name, 'data', 
                                  paste0(subject_name, '_files.sh')))
  write_lines(c('#!/bin/bash',
                '#PBS -P gx8',
                '#PBS -q copyq',
                '#PBS -l walltime=10:00:00',
                '#PBS -l mem=4GB',
                '#PBS -l ncpus=1',
                '#PBS -l software=bcbio',
                '#PBS -l wd',
                '#PBS -l storage=gdata/gx8',
                '',
                '# Adjust path to include ICA',
                'export PATH=/g/data/gx8/local/production/bin:/g/data3/gx8/local/production/bcbio/anaconda/bin:/opt/bin:/bin:/usr/bin:/opt/pbs/default/bin:/g/data/gx8/extras/iap/'),
              file_handle)

  # The actual file download
  write.table(paste('ica files download', gds_link$from, 
                    paste('./', gds_link$to, ' 2>&1 > /dev/null', sep='')),
              file = here::here('output', subject_name, 'data', paste0(subject_name, '_files.sh')),
              col.names = FALSE,
              row.names = FALSE,
              quote = FALSE,
              append = TRUE)
  
  # Adjust bcbio sample summary with new samplename (filename) information
  summary <- summary %>%
    right_join(gds_link, by='row_id') %>%
    select(-samplename, -from) %>%
    rename(samplename=to) %>%
    relocate(samplename) 

  write.table(summary,
               file = here::here('output', subject_name, 'data', paste0(subject_name, '.csv')),
               row.names = FALSE,
               col.names = TRUE,
               quote = FALSE,
               sep = ',')

    # Add project information for re-use on Gadi
  project_name <- unique(summary$project_name)
  write_file(project_name,
             here::here('output', subject_name, 'data', 'project_name.txt'))
  
}

# Create a bcbio template and rename columns for the YAML 
template <- bcbio %>%
  mutate(family_id = subject_id) %>%
  mutate(individual_id = subject_id) %>%
  mutate(run = paste('run', run, sep = '_')) %>%
  select(samplename = targetname,
         description,
         batch = subject_id,
         source,
         external_phenotype = phenotype, 
         family_id, individual_id, row_id,
         external_subject_id,
         external_sample_id,
         external_sample_name = sample_name,
         project_name,
         gds_key,
         run)

# Group by subject
unique_subjects <- as.vector(unique(template$individual_id))

for (subject in unique_subjects) {
  write_subject(subject, template)
}
```

### 5. Run projects on Raijin

Raijin now has a `config_bcbio_wts.sh` script in `/g/data/gx8/projects/std_workflow/scripts`. For now, the target path is hardcoded into the script, so I recommend copying it over to where data was rsync'd over from Spartan and editing the path accordingly. In brief, `config_bcbio_wts.sh` will:

* find all sample directories in the configured parent directory
* copy the `merge_wts.sh` script from `std_workflow` over and adjust it for each batch
* trigger the sample merging and generate the revised sample sheet
* configure a bcbio run using the revised (merged) sample sheet 

All that's left is then to kick off the actual runs:

> `find /g/data/gx8/projects/Hofmann_Test/ -name run.sh -execdir qsub {} \;`







