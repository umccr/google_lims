---
title: "UMCCR bcbio single patient config"
author: "Oliver Hofmann"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: readable
    toc: false
    toc_float: false
    code_folding: hide
---

```{r custom, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(tidyr)
library(rmarkdown)
library(dplyr)
library(DT)
library(readr)
library(forcats)
library(stringr)
library(janitor)
library(googledrive)
library(here)
library(skimr)
library(purrr)
```

## Introduction

Code snippets to generate file summaries (for rsync'ing around) and bcbio sample summaries based on a [Google Spreadsheet](https://docs.google.com/spreadsheets/d/1DwvyfVrgr5TIcYtGVXZeIWWRbld-nuX-4o4z2kZHNWs/edit#gid=0), aka the dreadful LIMS stand-in. This version differs from the `bcbioSetup.Rmd` in that it is meant for _single_ patient samples only, i.e., samples are only batched for individual patients not across a whole project.

### 1. Setting up project information

This currently requires specifying both project name and the type of sequencing data (`WGS`, `WTS`, `10X-WGS`, etc.) and will pull out all files of that type and project which do not have results associated with them yet. This should be generalized at some point to support sample extraction by Illumina RunID or by patient ID. The exact names can be copied from the `Project` and `Type` columns of the [Google-LIMS sheet](https://docs.google.com/spreadsheets/u/1/d/1aaTvXrZSdA1ekiLEpW60OeNq2V7D_oEMBzTgC-uDJAM/edit#gid=0). The `googledrive` framework requires authentication with oAuth. This can be done interactive, but storing a token simplifies the process; see the [googlesheet authentication vignette](https://rawgit.com/jennybc/googlesheets/master/vignettes/managing-auth-tokens.html) for details. 

An alternative use is to set the "secondary" analysis flag to match the samples that need to be processed, regardless of processing status. The `PROJECT` name will be used to name config and sample files, but any sample with the matching `SECONDARY` entry in the `secondary analysis` column of the spreadsheet will be added as long as it is of the right `TYPE`. This is useful when re-processing samples for research projects. Long term, the idea is that we get rid of this filtering step completely and just generate sync lists and templates for all samples that still need to be processed, then mark the processing stage in Google-LIMS to avoid duplication.

```{r project}
PROJECT <- 'Kolling-Research'
TYPE <- 'WGS'
SECONDARY <- 'share'
```

### 2. Import data from Google spreadsheet

This step generates a data frame from the Google spreadsheet, unifying variable names and simplifying subject IDs for bcbio along the way. It replaces empty `results` cells (which indicate samples that still need processing) with a `-` to distinguish from true NAs and gets rid of whitespace in subject identifiers (although ideally there should not be any empty cells in the sheet in the first place). Otherwise standard data cleanup with `janitor`. We are also creating a timestamped backup of the Google Doc each time it is accessed, just in case.

```{r importData, message=FALSE}
# Create a backup copy each time we access the sample info
tm <- as.POSIXlt(Sys.time(), "UTC", "%Y-%m-%dT%H:%M:%S")
timestamp <- strftime(tm , "%Y-%m-%dT%H%M")
filename <- paste(timestamp, 'backup.csv', sep = '_')
backup_dir <- file.path(here::here('output', 'backup'))
dir.create(backup_dir, recursive = TRUE) # mkdir -p (warns if it already exists):

# Google Drive implementation
lims_key <- drive_find('^Google LIMS$', team_drive = 'LIMS')$id
drive_download(as_id(lims_key), path = file.path(backup_dir, filename), overwrite = TRUE)

# Import downloaded spreadsheet (sheet #1) as a tibble
samples_gs <- read_csv(file.path(backup_dir, filename))

# Tweak for analysis
samples <- samples_gs %>%
  clean_names() %>%
  mutate(secondary_analysis = ifelse(is.na(secondary_analysis), '-', secondary_analysis)) %>%
  remove_empty(c('rows', 'cols')) %>%
  mutate(subject_id = gsub(' ', '.', subject_id)) %>%
  mutate(results = ifelse(is.na(results), '-', results)) %>%
  select(-matches('number_fastqs')) # Drop FASTQ count introduced with new version for now

# Add unique id to each row
samples <- samples %>%
  mutate(row_id = rownames(samples))
```

### 3. Find files ready for processing

Find all samples that a) still need to be processed, and b) for which we have both tumor and normal sample ready to go. 

```{r subsetSamples}
# Keep rows matching project and type requested; extract path to FASTQ
# and generate new file name for these
bcbio <- samples %>%
  filter(type == TYPE &
           ((project == PROJECT & results == '-') | 
           (secondary_analysis == SECONDARY))) %>% 
  select(illumina_id, fastq, run, project, sample_id, sample_name, subject_id, 
         phenotype, row_id) %>%
  mutate(runname = illumina_id) %>%
  select(-illumina_id)

# Starting with run 108 we switched SampleID and SampleName. For naming conventions 
# sticking to the old format when deciding what to call the FASTQs
bcbio <- bcbio %>%
  mutate(targetname = case_when(run < 108 ~ paste(runname, sample_name, sep = '_'),
                                run >= 108 ~ paste(runname, sample_id, sep= '_'))) %>%
  mutate(targetname = str_replace_all(targetname, '-', '_'))
```


### 4. Group samples

Assigning patient/family identifiers for Peddy, assigning batches based on the subject id, and pairing tumor/normals as needed. Samples with the exact same name (but from different runs) are expected to be top-ups and will subsequently be merged using `bcbio_prepare_samples.py`. Finally, remove samples for which we do not have a matched normal.

```{r assignBatches}
# Handle cases where a normal matches more than one tumor sample.
# Courtesy of [Peter Diakumis](https://github.com/pdiakumis)
get_new_batch <- function(batch, phenotype) {
  stopifnot(length(batch) == length(phenotype))
  n <- length(batch)

  new_batch <- vector(mode = "character", length = n)
  x <- table(phenotype)

  if (n > 2) {
    tum_ns <- seq_len(x["tumor"])
    new_batch[phenotype == "tumor"] <- paste0(batch[phenotype == "tumor"], '_', tum_ns)
    # assume only one normal for now
    new_batch[phenotype == "normal"] <- paste0(batch[phenotype == "tumor"], '_', tum_ns, collapse = ";")
  } else {
    new_batch <- batch
  }
  return(new_batch)
}


# Generating the bcbio YAML template. Again, after run 107 use the SampleName instead
# of SampleID to label things. Keep the required columns and add information for peddy
#
# Remove top-up samples (samples with the exact same description) before calculating
# batch assignments. We'll add them back in later. This assumes topups are consistently
# flagged with a `_topup` suffix
template <- bcbio %>% 
  mutate(description = case_when(run < 108 ~ sample_name,
                               run >= 108 ~ sample_id)) %>%
  mutate(description = str_replace(description, '_topup', '')) %>%
  mutate(family_id = subject_id) %>%
  mutate(individual_id = subject_id) %>%
  select(samplename = targetname,
         description,
         batch = subject_id,
         phenotype, family_id, individual_id, row_id)

# Only keep one sample row for samples that _do_ have topups
unique_samples <- template %>%
  select(-row_id) %>%
  group_by(description) %>%
  filter(row_number(description) == 1)

# Identify batches for which we have both tumor and tumor samples
# Well. Technically just testing that we have two distinct phenotypes.
# Could be made more specific (e.g., by ensuring that 'normal' is
# present).
complete_subjects <- template %>%
  group_by(individual_id, batch) %>%
  summarise(phenotype_count = n_distinct(phenotype)) %>%
  filter(phenotype_count >= 2) %>%
  select(individual_id)

complete_subjects <- as.vector(complete_subjects$individual_id)

# Create batch information
batches <- unique_samples %>%
  group_by(batch) %>%
  mutate(new_batch = get_new_batch(batch, phenotype)) 

# Bring back the batch information to the main template
template <- template %>%
  left_join(batches, by = c('description')) %>%
  select(samplename = samplename.x,
         description,
         batch = new_batch,
         phenotype = phenotype.x,
         family_id = family_id.x,
         individual_id = individual_id.x,
         row_id) 

# Only retain samples that fulfill the 'complete batch' requirement
template <- template %>%
  filter(individual_id %in% complete_subjects)
```

### 5. Generating bcbio csv templates on a per-subject basis:

Generate a file with pointers to the sample FASTQs and their preferred new names as well as a sample descriptor (`.csv`) for bcbio to use as part of it's templating approach. The resulting directory structure can be copied to Spartan, e.g.:

> `scp -r BATCHFILES spartan:/data/cephfs/punim0010/data/Transfer/raijin/`

```{r perBatch}
write_subject <- function(complete, summary) {
  # Create a unique project name
  subject_name <- paste0(timestamp, '_', PROJECT, '_', TYPE, '_', complete)

  # Set up the project directory
  dir.create(here::here('output', subject_name, 'data'), showWarnings=FALSE, recursive=TRUE)
     
  # Subset to one individual
  summary <- summary %>%
    filter(individual_id == complete)

  # Find corresponding entries in the Google-Lims
  bcbio_subset <- bcbio %>%
     filter(row_id %in% summary$row_id)

  # Create a file list ready for linking
  #
  # With Run 81 the file structure from `bcl2fastq` changed. FASTQs are no
  # longer round in the top level run folder, but in
  # ```<Illumina_ID>/<Project>/<SampleName>/<SampleID>_R1_001.fastq.gz```
  read1 <- bcbio_subset %>%
    mutate(from = case_when(run <= 80 ~ paste0(fastq, '/', sample_id, '_R1_001.fastq.gz'),
                            run > 80 & run <= 93 ~ paste0(fastq, '/', project, '/', sample_name, '/', sample_id, '_R1_001.fastq.gz'),
                            run > 93 ~ str_replace(fastq, '.fastq.gz', '_R1_001.fastq.gz'))) %>%
    mutate(to = paste0(targetname, '_R1_001.fastq.gz')) %>%
    select(from, to)
  
  read2 <- bcbio_subset %>%
    mutate(from = case_when(run <= 80 ~ paste0(fastq, '/', sample_id, '_R2_001.fastq.gz'),
                            run > 80 & run <= 93 ~ paste0(fastq, '/', project, '/', sample_name, '/', sample_id, '_R2_001.fastq.gz'),
                            run > 93 ~ str_replace(fastq, '.fastq.gz', '_R2_001.fastq.gz'))) %>%
    mutate(to = paste0(targetname, '_R2_001.fastq.gz')) %>%
    select(from, to)
  
  link <- rbind(read1, read2)

  # Adapt to S3 syntax which doesn't include wildcards; split path and filename for include/exclude statement
  link <- link %>%
    mutate(from_path = dirname(link$from)) %>%
    mutate(from_file = basename(link$from))

  # Write the file list for staging files. First step, `aws cp` 
  write.table(paste('aws --profile spartan_s3_readonly s3 cp', 
                    link$from_path,
                    '.',
                    '--recursive --exclude "*" --include',
                    paste('"', link$from_file, '"', sep = '')),
              file = here::here('output', subject_name, 'data', paste0(subject_name, '_files.sh')),
              col.names = FALSE,
              row.names = FALSE,
              quote = FALSE)

  # Second step, rename
  write.table(paste('mv', 
                    link$from_file,
                    link$to,
                    sep = ' '),
              file = here::here('output', subject_name, 'data', paste0(subject_name, '_files.sh')),
              col.names = FALSE,
              row.names = FALSE,
              quote = FALSE,
              append = TRUE)

  # Write CSV for bcbio
  summary_R1 <- summary %>% mutate(samplename = paste0(samplename, '_R1_001.fastq.gz'))
  summary_R2 <- summary %>% mutate(samplename = paste0(samplename, '_R2_001.fastq.gz'))
  summary <- rbind(summary_R1, summary_R2)

  write.table(summary,
              file = here::here('output', subject_name, 'data', paste0(subject_name, '.csv')),
              row.names = FALSE,
              col.names = TRUE,
              quote = FALSE,
              sep = ',')
}

for (subject in complete_subjects) {
  write_subject(subject, template)
}
```


<!-- Removing this section for now. Partial to sharing data with the standard per-sample approach instead. -->

<!-- ### 6. Sharing data only -->

<!-- In some cases we only need to distribute FASTQs. This will generate the link information to gather all relevant files in one place. -->


<!-- ```{r shareData, eval=FALSE} -->
<!-- #bcbio_subset <- bcbio %>% -->
<!-- #  filter(row_id %in% template$row_id) -->
<!-- project_name <- paste0(timestamp, '_', PROJECT, '_', TYPE, '_Distribute') -->

<!-- read1 <- bcbio %>% -->
<!--   mutate(from = case_when(run <= 80 ~ paste0(fastq, '/', sample_id, '_R1_001.fastq.gz'), -->
<!--                           run > 80 & run <= 93 ~ paste0(fastq, '/', project, '/', sample_name, '/', sample_id, '_R1_001.fastq.gz'), -->
<!--                           run > 93 ~ str_replace(fastq, '.fastq.gz', '_R1_001.fastq.gz'))) %>% -->
<!--   mutate(to = paste0(targetname, '_R1_001.fastq.gz')) %>% -->
<!--   select(from, to) -->

<!-- read2 <- bcbio %>% -->
<!--   mutate(from = case_when(run <= 80 ~ paste0(fastq, '/', sample_id, '_R2_001.fastq.gz'), -->
<!--                           run > 80 & run <= 93 ~ paste0(fastq, '/', project, '/', sample_name, '/', sample_id, '_R2_001.fastq.gz'), -->
<!--                           run > 93 ~ str_replace(fastq, '.fastq.gz', '_R2_001.fastq.gz'))) %>% -->
<!--   mutate(to = paste0(targetname, '_R2_001.fastq.gz')) %>% -->
<!--   select(from, to) -->

<!-- link <- rbind(read1, read2) -->

<!-- # Adapt to S3 syntax which doesn't include wildcards; split path and filename for include/exclude statement -->
<!-- link <- link %>% -->
<!--   mutate(from_path = dirname(link$from)) %>% -->
<!--   mutate(from_file = basename(link$from)) -->

<!-- # Write the file list for staging files. First step, `aws cp`  -->
<!-- write.table(paste('aws --profile spartan_s3_readonly s3 cp',  -->
<!--                   link$from_path, -->
<!--                   '.', -->
<!--                   '--recursive --exclude "*" --include', -->
<!--                   paste('"', link$from_file, '"', sep = '')), -->
<!--             file = here::here('output', project_name, 'data', paste0(project_name, '_files.sh')), -->
<!--             col.names = FALSE, -->
<!--             row.names = FALSE, -->
<!--             quote = FALSE) -->

<!--   # Second step, rename -->
<!--   write.table(paste('mv',  -->
<!--                     link$from_file, -->
<!--                     link$to, -->
<!--                     sep = ' '), -->
<!--               file = here::here('output', subject_name, 'data', paste0(subject_name, '_files.sh')), -->
<!--               col.names = FALSE, -->
<!--               row.names = FALSE, -->
<!--               quote = FALSE, -->
<!--               append = TRUE) -->



<!-- # Write the file list for linking on Spartan -->
<!-- write.table(paste('ln', link$from, link$to, sep = ' '), -->
<!--             file = here::here('output', paste0(timestamp, PROJECT, 'files.sh', sep='_')), -->
<!--             col.names = FALSE, -->
<!--             row.names = FALSE, -->
<!--             quote = FALSE) -->
<!-- ``` -->


### 7. Sync data from Spartan to Raijin

WGS/WTS runs are being processed at Raijin. Collect the required file in one place and rsync over with:

> `find /data/cephfs/punim0010/data/Transfer/raijin/ -name *files.sh* -execdir sh {} \;`
> `rsync -aPL --append-verify --remove-source-files /data/cephfs/punim0010/data/Transfer/raijin/ omh563@r-dm.nci.org.au:/g/data3/gx8/projects/Hofmann_Workflow`
> `find /data/cephfs/punim0010/data/Transfer/raijin/* -type d -empty -delete`

### 8. Run projects on Raijin

Raijin now has a `config_bcbio.sh` script in `/g/data/gx8/projects/std_workflow`. For now, the target path is hardcoded into the script, so I recommend copying it over to where data was rsync'd over from Spartan and editing the path accordingly. In brief, `config_bcbio.sh` will:

* find all sample directories in the configured parent directory
* copy the `merge.sh` script from `std_workflow` over and adjust it for each batch
* trigger the sample merging and generate the revised sample sheet
* configure a bcbio run using the revised (merged) sample sheet 

All that's left is then to kick off the actual runs:

> `find /g/data/gx8/projects/Hofmann_Test/ -name run.sh -execdir qsub {} \;`







