---
title: "UMCCR Sample Summary"
author: "Oliver Hofmann"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: readable
    toc: false
    toc_float: false
    code_folding: hide
---

```{r custom, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(scales)
library(gridExtra)
library(gtools)
library(RColorBrewer)
library(knitr)
library(tidyr)
library(rmarkdown)
library(dplyr)
library(DT)
library(readr)
library(forcats)
library(stringr)
library(janitor)
library(googledrive)
library(here)
library(skimr)
library(purrr)
```

## Introduction

Code snippets to generate file summaries (for rsync'ing around) and bcbio sample summaries based on a [Google Spreadsheet](https://docs.google.com/spreadsheets/d/1DwvyfVrgr5TIcYtGVXZeIWWRbld-nuX-4o4z2kZHNWs/edit#gid=0), aka the dreadful LIMS stand-in.


## Workflow

The process is currently manual, i.e., run code block by code block in RStudio as we are still trying to sort out edge cases. It is also useful being able to inspect the data frames listing files, phenotypes and batches before kicking off a bcbio run. A typical workflow includes the steps below. 

### 1. Setting up project information

This currently requires specifying both project name and the type of sequencing data (`WGS`, `WTS`, `10X-WGS`, etc.) and will pull out all files of that type and project which do not have results associated with them yet. This should be generalized at some point to support sample extraction by Illumina RunID or by patient ID. The exact names can be copied from the `Project` and `Type` columns of the [Google-LIMS sheet](https://docs.google.com/spreadsheets/d/1DwvyfVrgr5TIcYtGVXZeIWWRbld-nuX-4o4z2kZHNWs/edit?usp=sharing).

The Googlesheets framework requires authentication with oAuth. This can be done interactive, but storing a token simplifies the process; see the [googlesheet authentication vignette](https://rawgit.com/jennybc/googlesheets/master/vignettes/managing-auth-tokens.html) for details. 

An alternative use is to set the "secondary" analysis flag to match the samples that need to be processed, regardless of processing status. The `PROJECT` name will be used to name config and sample files, but any sample with the matching `SECONDARY` entry in the `secondary analysis` column of the spreadsheet will be added as long as it is of the right `TYPE`. This is useful when re-processing samples for research projects.

```{r project, eval=FALSE}
PROJECT <- ''
TYPE <- 'WGS'
SECONDARY <- 'Cori'
```

### 2. Import data from Google spreadsheet

This step generates a data frame from the Google spreadsheet, unifying variable names and simplifying subject IDs for bcbio along the way. It replacesg empty `results` cells (which indicate samples that still need processing) with a `-` to distinguish from true NAs and gets rid of whitespace in subject identifiers. Otherwise standard data cleanup with `janitor`. We are also creating a timestamped backup of the Google Doc each time it is accessed, just in case.

```{r importData, message=FALSE, eval=FALSE}
# Register UMCCR spreadsheet. Use cached authentication
gs_auth(token = "./googlesheets_token_umccr.rds") # plain gs_auth() writes .httr-oauth to workdir, then all is good

# Create a backup copy each time we access the sample info
tm <- as.POSIXlt(Sys.time(), "UTC", "%Y-%m-%dT%H:%M:%S")
timestamp <- strftime(tm , "%Y-%m-%dT%H%M")
filename <- paste(timestamp, 'backup.csv', sep = '_')
backup_dir <- file.path(here::here('output', 'backup'))
dir.create(backup_dir, recursive = TRUE) # mkdir -p (warns if it already exists):

# Google Drive implementation
lims_key <- drive_find('^Google LIMS$', team_drive = 'LIMS')$id
drive_download(as_id(lims_key), path = file.path(backup_dir, filename), overwrite = TRUE)

# Import downloaded spreadsheet (sheet #1) as a tibble
samples_gs <- read_csv(file.path(backup_dir, filename))

# Tweak for analysis
samples <- samples_gs %>%
  clean_names() %>%
  mutate(secondary_analysis = ifelse(is.na(secondary_analysis), '-', secondary_analysis)) %>%
  remove_empty(c('rows', 'cols')) %>%
  mutate(subject_id = gsub(' ', '.', subject_id)) %>%
  mutate(results = ifelse(is.na(results), '-', results)) %>%
  select(-matches('number_fastqs')) # Drop FASTQ count introduced with new version for now

# Add unique id to each row
samples <- samples %>%
  mutate(row_id = rownames(samples))
```


### 3. Create rsync list

Find all FASTQs for samples that still need to be processed. The generated output file can be used on Spartan to generate (hard)links ready for `rsync` to NCI or `aws s3 cp` to, well, S3. It also generates 'better' sample names by renaming the FASTQ files from the filename to the sample name.

```{r csv, eval=FALSE}
# Keep rows matching project and type requested; extract path to FASTQ
# and generate new file name for these
bcbio <- samples %>%
  filter(type == TYPE &
           ((project == PROJECT & results == '-') | 
           (secondary_analysis == SECONDARY))) %>%
  select(fastq, run, project, sample_id, sample_name, subject_id, phenotype) %>%
  mutate(runname = basename(fastq)) %>%
  mutate(targetname = paste(runname, sample_name, sep = '_')) %>%
  mutate(targetname = str_replace_all(targetname, '-', '_'))

# Create a file list ready for linking
#
# With Run 81 the file structure from `bcl2fastq` changed. FASTQs are no
# longer round in the top level run folder, but in
# ```<Illumina_ID>/<Project>/<SampleName>/<SampleID>_R1_001.fastq.gz```
read1 <- bcbio %>%
  mutate(from = ifelse(run<= 80,
                       paste0(fastq, '/', sample_id, '_R1_001.fastq.gz'),
                       paste0(fastq, '/', project, '/', sample_name, '/', sample_id, '_R1_001.fastq.gz'))) %>%
  mutate(to = paste0(targetname, '_R1_001.fastq.gz')) %>%
  select(from, to)

read2 <- bcbio %>%
  mutate(from = ifelse(run<= 80,
                       paste0(fastq, '/', sample_id, '_R2_001.fastq.gz'),
                       paste0(fastq, '/', project, '/', sample_name, '/', sample_id, '_R2_001.fastq.gz'))) %>%
  mutate(to = paste0(targetname, '_R2_001.fastq.gz')) %>%
  select(from, to)

link <- rbind(read1, read2)

# Write the file list for linking on Spartan
# ** Not sure if a `.sh` extension would make it clearer.
write.table(paste('ln', link$from, link$to, sep = ' '),
            file = here::here('output', paste0(timestamp, '_', PROJECT, '_', TYPE, '_files.txt')),
            col.names = FALSE,
            row.names = FALSE,
            quote = FALSE)
```

### 4. Create bcbio config: WGS tumor/normal use case

Generate a `.csv` sample descriptor, assigning patient/family identifiers for Peddy, assigning batches based on the subject id, and pairing tumor/normals as needed. Samples with the exact same name (but from different runs) are expected to be top-ups and will subsequently be merged using `bcbio_prepare_samples.py`.

```{r bcbioconfig, eval=FALSE}
# Use the same information to generate the bcbio CSV file
# Keep the required columns and add information for peddy
template <- bcbio %>% 
  mutate(family_id = subject_id) %>%
  mutate(individual_id = subject_id) %>%
  select(samplename = targetname,
         description = sample_name,
         batch = subject_id,
         phenotype, family_id, individual_id)

# Handle cases where a normal matches more than one tumor sample.
# Courtesy of [Peter Diakumis](https://github.com/pdiakumis)
get_new_batch <- function(batch, phenotype) {
  stopifnot(length(batch) == length(phenotype))
  n <- length(batch)

  new_batch <- vector(mode = "character", length = n)
  x <- table(phenotype)

  if (n > 2) {
    tum_ns <- seq_len(x["tumor"])
    new_batch[phenotype == "tumor"] <- paste0(batch[phenotype == "tumor"], '_', tum_ns)
    # assume only one normal for now
    new_batch[phenotype == "normal"] <- paste0(batch[phenotype == "tumor"], '_', tum_ns, collapse = ";")
  } else {
    new_batch <- batch
  }
  return(new_batch)
}

# Remove top-up samples (samples with the exact same description)
# before calculating batch assignments. We'll add them back in later.
# This assumes topups are consistently flagged with a `_topup`
# suffix
template <- template %>%
  mutate(description = str_replace(description, '_topup', ''))

# Only keep one sample row for samples that _do_ have topups
unique_samples <- template %>%
  group_by(description) %>%
  filter(row_number(description) == 1)

# Create batch information
batches <- unique_samples %>%
  group_by(batch) %>%
  mutate(new_batch = get_new_batch(batch, phenotype)) 

# Bring back the batch information to the main template
template <- template %>%
  left_join(batches, by = c('description')) %>%
  select(samplename = samplename.x,
         description,
         batch = new_batch,
         phenotype = phenotype.x,
         family_id = family_id.x,
         individual_id = individual_id.x)

# Since we are merging add the R1/R2 readpair information. Could probably 
# be more elegant...
template_R1 <- template %>% mutate(samplename = paste0(samplename, '_R1_001.fastq.gz'))
template_R2 <- template %>% mutate(samplename = paste0(samplename, '_R2_001.fastq.gz'))
template <- rbind(template_R1, template_R2)

write.table(template,
            file = here::here('output', paste0(timestamp, '_', PROJECT, '_', TYPE, '.csv')),
            row.names = FALSE,
            col.names = TRUE,
            quote = FALSE,
            sep = ',')
```

### 5. Data set generation

Assuming a run on Spartan:

* create a new project directory on Spartan, with a `data` subdirectory (`mkdir -p /data/cephfs/punim0010/projects/<myproject>/data; cd /data/cephfs/punim0010/projects/<myproject>/data`)
* copy over the two newly generated files to Spartan (`scp TIMESTAMP_PROJECT* spartan:/data/cephfs/punim0010/projects/<myproject>/data`)
* link required files (`sh TIMESTAMP_PROJECT_files.txt`)

This should result in all required FASTQ files being present in the current data directories. Errors usually mean permissions need to be updated, or the sample name is incorrectly captured in the spreadsheet. The next step merges FASTQ files where necessary and generates the final sample manifest for bcbio:

* Copy over the merge script (`cp /data/cephfs/punim0010/projects/std_workflow/merge.sh .`) and edit the `--csv` flag to use the generated sample sheet (`TIMESTAMP_PROJECT_files.csv`)
* Submit the job to the scheduler (`sbatch merge.sh`)


**NOTE**: if the scheduler above doesn't work for some reason, you can just connect to an interactive session and simply run:

```
bcbio_prepare_samples.py --out merged --csv 2018-10-10T2348_Tothill-Research_WGS.csv -n32 # I don't think you really need 32 cores; just go with as many FASTQs you have
```


This will result in a new `merged` CSV alongside our standard template:

* `mv *-merged.csv ..; cd ..`
* `bcbio_nextgen.py -w template ../std_workflow/std_workflow_cancer.yaml TIMESTAMP_PROJECT-merged.csv data/merged/*.gz`
* `cp ../std_workflow/run.sh TIMESTAMP_PROJECT-merged/work; cd TIMESTAMP_PROJECT-merged/work`

Edit the `run.sh` job submission script to point at the newly generated bcbio YAML template and you should be good to go with `sbatch run.sh`.

### 6. Monitoring a run

The easiest way to follow the progress of a run is with a `less +F log/bcbio-nextgen-debug.log`; if this gets too much just following the commands log in the same directory would also do the trick.

### 7. Post-processing with [umccrise](https://github.com/umccr/umccrise)

Once the bcbio run finishes, you need to run the `umccrise` post-processing script. 
Make sure you follow the directions [in the wiki](https://github.com/umccr/wiki/blob/master/computing/cloud/aws.md)
for setting up your AWS credentials, in order to upload mini-BAM files to AWS for IGV viewing.

Starting from the `work` directory where `run.sh` is placed:

* On Raijin:

```bash
cd ..
source /g/data3/gx8/extras/umccrise/load_umccrise.sh
export AWS_PROFILE=umccr
umccrise . -j 16 --cluster-auto` # by default outputs to 'umccrised'
## or use `umccrise /path/to/bcbio/final -j 16 --cluster-auto -o umccrised_alternative_name`
```

* On Spartan:

```bash
cd ..;
source /data/cephfs/punim0010/extras/umccrise/load_umccrise.sh
export AWS_PROFILE=umccr
umccrise . -j 16 --cluster-auto` # by default outputs to 'umccrised'
## or use `umccrise /path/to/bcbio/final -j 16 --cluster-auto -o umccrised_alternative_name`
```

The post-processing will take about an hour per sample, mostly due to creating mini-BAM files from the full BAMs.


### 8. Archival and records

Our current data archive resides on Spartan, although most of the processed data is now also on AWS S3. For now the process still is:

* `/data/cephfs/punim0010/data/Results`
* create `PROJECTDIR` if it does not exist, using the project name listed in the google-lims spreadsheet; it should match the project name that has been used to process the samples
* `cd PROJECTDIR` and create a timestamp matching the timestamp of bcbio's `final` directory. E.g., if a `ls final` includes `2018-08-11_2018-07-31T0005_Tothill-A5_WGS-merged` create a directory `2018-08-11`
* move (or rsync) the `final`, `config` and `umccrised` directories into the newly created timestamp directory
    * delete the `work` directory.
    * delete the project directory you created, along with its `data` subdirectory.
* record the full path including the timestamp in the `Results` column of the Google Docs spreadsheet
* move the [Trello cards](https://trello.com/b/05zkoAZU/umccr-patients) matching the current samples to the new state, ideally also uploading QC reports, PCGR reports, and the Rmd summary for each sample


## Alternative workflows

### WGS Positive Controls

Positive controls use a slightly different template (and only require germline calls); we also do not merge files prior to processing. We do add a timestamp to the batch based on the Illumina Run Identifier to be abe to sort / filter by time of the run in MultiQC and define the gold standard / benchmarkfiles to use for `rtg eval`:

```{r posControlFiles, eval=FALSE}
# Define what validation data to use; all of these are
# shipped with bcbio so we don't have to prep standards
validations <- c('giab-NA12878', 'giab-NA24385', 'giab-NA24631')
names(validations) <- c('NA12878', 'NA24385', 'NA24631')

# As before, generate bcbio CSV. Simpler process here.
template <- bcbio %>% 
  mutate(family_id=subject_id) %>%
  mutate(individual_id=subject_id) %>%
  select(samplename=targetname, 
         batch=subject_id,
         family_id, individual_id)

# Add timestamp to description, batch
template <- template %>%
  separate(samplename, c('timestamp', 'machine', 'run'),
           sep='_', extra="drop", 
           remove=FALSE) %>%
  mutate(description=paste(timestamp, machine, run, batch, sep='-')) %>%
  mutate(batch=paste('batch', timestamp, machine, run, batch, sep='-')) %>%
    select(samplename, description, batch, family_id, individual_id)

# Add the validation information
template <- template %>% 
  mutate(validate=paste(validations[individual_id], 
                        'truth_small_variants.vcf.gz', sep='/')) %>%
  mutate(validate_regions=paste(validations[individual_id], 
                                'truth_regions.bed', sep='/')) %>%
  mutate(validate_batch=individual_id)

write.table(template,
            file = here::here('output', paste0(timestamp, '_', PROJECT, '_', TYPE, '.csv')),
            row.names = FALSE,
            col.names = TRUE,
            quote = FALSE,
            sep = ',')

```

### WGS Negative Controls

For negative controls we only need to collect all relevant files in one place (hardlinks again) and check the results with `find . -type f -size +500M -exec ls -sh {} \; 2> /dev/null`.

```{r negControl, eval=FALSE}
# rsync command as before
bcbio <- samples %>%
  filter(project == 'Negative-control') %>%
  mutate(targetname = paste(illumina_id, sample_name, sep='_')) %>%
  select(fastq, sample_id, sample_name, targetname, subject_id, phenotype)

# Create a file list ready for linking
read1 <- bcbio %>%
  mutate(from = paste(fastq, '/', sample_id, '_R1_001.fastq.gz', sep='')) %>%
  mutate(to = paste(targetname, '_R1_001.fastq.gz', sep='')) %>%
  select(from, to)

read2 <- bcbio %>%
  mutate(from = paste(fastq, '/', sample_id, '_R2_001.fastq.gz', sep='')) %>%
  mutate(to = paste(targetname, '_R2_001.fastq.gz', sep='')) %>%
  select(from, to)

link <- rbind(read1, read2)

# Write the file list for linking
write.table(paste('ln', link$from, link$to, sep=' '),
            file=here::here('output', paste(timestamp, '_negControl_files.txt', sep='')),
            col.names=FALSE,
            row.names=FALSE,
            quote=FALSE)
```

### WTS

For WTS samples the process is easier since we do not need to handle phenotypes or batch pairing:

```{r rnasync, eval=FALSE}
# Use the same information to generate the bcbio CSV file
# Keep the required columns and add information for peddy
template <- bcbio %>% 
  mutate(family_id=subject_id) %>%
  mutate(individual_id=subject_id) %>%
  select(samplename=targetname, 
         description=sample_name,
         genotype=run)

# Since we are merging add the R1/R2 readpair information. Could probably 
# be more elegant...
template_R1 <- template %>%
  mutate(samplename = paste(samplename, '_R1_001.fastq.gz', sep=''))
template_R2 <- template %>%
  mutate(samplename = paste(samplename, '_R2_001.fastq.gz', sep=''))
template <- rbind(template_R1, template_R2)

write.table(template,
            file=here::here('output', paste(timestamp, '_', 
                                            PROJECT, '_', 
                                            TYPE, '.csv', sep='')),
            row.names=FALSE,
            col.names=TRUE,
            quote=FALSE,
            sep=',')
```


### Create bcbio config (positive WTS controls)

Different configuration for WTS controls -- we do not validate against a benchmark / standard. 

```{r posControlFilesWTS, eval=FALSE}
# rsync command as before
bcbio <- samples %>%
  filter(project == 'Positive-control' & 
           type == 'WTS' &
           results == '-') %>%
  mutate(targetname = paste(illumina_id, sample_name, sep='_')) %>%
  select(fastq, sample_id, sample_name, targetname, subject_id, phenotype)

# Create a file list ready for linking
read1 <- bcbio %>%
  mutate(from = paste(fastq, '/', sample_id, '_R1_001.fastq.gz', sep='')) %>%
  mutate(to = paste(targetname, '_R1_001.fastq.gz', sep='')) %>%
  select(from, to)

read2 <- bcbio %>%
  mutate(from = paste(fastq, '/', sample_id, '_R2_001.fastq.gz', sep='')) %>%
  mutate(to = paste(targetname, '_R2_001.fastq.gz', sep='')) %>%
  select(from, to)

link <- rbind(read1, read2)

# Write the file list for linking
write.table(paste('ln', link$from, link$to, sep=' '),
            file=here::here('output', 'posControl_WTS_files.txt'),
            col.names=FALSE,
            row.names=FALSE,
            quote=FALSE)

# As before, generate bcbio CSV. Simpler process here.
template <- bcbio %>% 
  mutate(family_id=subject_id) %>%
  mutate(individual_id=subject_id) %>%
  select(samplename=targetname, 
         batch=subject_id,
         phenotype, family_id, individual_id)

# Add timestamp to description, batch
template <- template %>%
  separate(samplename, c('timestamp', 'machine', 'run'),
           sep='_', extra="drop", 
           remove=FALSE) %>%
  mutate(description=paste(timestamp, machine, run, batch, sep='-')) %>%
  mutate(batch=paste('batch', timestamp, machine, run, batch, sep='-')) %>%
    select(samplename, description, batch, phenotype,
         family_id, individual_id)

# Since we are merging add the R1/R2 readpair information. Could probably 
# be more elegant...
template_R1 <- template %>%
  mutate(samplename = paste(samplename, '_R1_001.fastq.gz', sep=''))
template_R2 <- template %>%
  mutate(samplename = paste(samplename, '_R2_001.fastq.gz', sep=''))
template <- rbind(template_R1, template_R2)

write.table(template,
           file=here::here('output', 'posControl_WTS.csv'),
           row.names=FALSE,
           col.names=TRUE,
           quote=FALSE,
           sep=',')
```


