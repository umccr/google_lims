---
title: "UMCCR bcbio single patient config"
author: "Oliver Hofmann"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: readable
    toc: false
    toc_float: false
    code_folding: hide
---

```{r custom, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(tidyr)
library(rmarkdown)
library(dplyr)
library(DT)
library(readr)
library(forcats)
library(stringr)
library(janitor)
library(googledrive)
library(here)
library(skimr)
library(purrr)
```

## Introduction

Code snippets to generate file summaries (for rsync'ing around) and bcbio sample summaries based on a [Google Spreadsheet](https://docs.google.com/spreadsheets/d/1DwvyfVrgr5TIcYtGVXZeIWWRbld-nuX-4o4z2kZHNWs/edit#gid=0), aka the dreadful LIMS stand-in. This version differs from the `bcbioSetup.Rmd` in that it is meant for _single_ **germline** samples only, i.e., samples are only batched for individual patients not across a whole project. This should be the default approach.

### 1. Setting up project information

This currently requires specifying either the project name or a 'secondary use' field. The exact names can be copied from the `Project` column of the Google-LIMS. An alternative use is to set the "secondary" analysis flag to match the samples that need to be processed, regardless of processing status. The `PROJECT` name will be used to name config and sample files, but any sample with the matching `SECONDARY` entry in the `secondary analysis` column of the spreadsheet will be added as long as it is of the right `TYPE` (WGS for this script). This is useful when re-processing samples for research projects, or when wanting to process a new NovaSeq run with samples from multiple different projects. 

The `googledrive` framework requires authentication with oAuth. This can be done interactive, but storing a token simplifies the process; see the [googlesheet authentication vignette](https://rawgit.com/jennybc/googlesheets/master/vignettes/managing-auth-tokens.html) for details. 

```{r project}
PROJECT <- 'All'
SECONDARY <- ''

# Do not change
TYPE <- 'WGS'
```

### 2. Import data from Google spreadsheet

This step generates a data frame from the Google spreadsheet, unifying variable names and simplifying subject IDs for bcbio along the way. It replaces empty `results` cells (which indicate samples that still need processing) with a `-` to distinguish from true NAs and gets rid of whitespace in subject identifiers (although ideally there should not be any empty cells in the sheet in the first place). Otherwise standard data cleanup with `janitor`. We are also creating a timestamped backup of the Google Doc each time it is accessed, just in case.

```{r importData, message=FALSE}
# Create a backup copy each time we access the sample info
tm <- as.POSIXlt(Sys.time(), "UTC", "%Y-%m-%dT%H:%M:%S")
timestamp <- strftime(tm , "%Y-%m-%dT%H%M")
filename <- paste(timestamp, 'backup.csv', sep = '_')
backup_dir <- file.path(here::here('output', 'backup'))
dir.create(backup_dir, recursive = TRUE) # mkdir -p (warns if it already exists):

# Google Drive implementation
lims_key <- drive_find('^Google LIMS$', team_drive = 'LIMS')$id
drive_download(as_id(lims_key), path = file.path(backup_dir, filename), overwrite = TRUE)

# Import downloaded spreadsheet (sheet #1) as a tibble
samples_gs <- read_csv(file.path(backup_dir, filename))

# Tweak for analysis
#
# Currently mapping external_subject_id to subject_id (and same for sample id)
#
samples <- samples_gs %>%
  clean_names() %>%
  mutate(secondary_analysis = ifelse(is.na(secondary_analysis), '-', secondary_analysis)) %>%
  remove_empty(c('rows', 'cols')) %>%
  mutate(results = ifelse(is.na(results), '-', results)) %>%
  select(-matches('number_fastqs')) # Drop FASTQ count introduced with new version for now

# Add unique id to each row
samples <- samples %>%
  mutate(row_id = rownames(samples))
```

### 3. Find files ready for processing

Find all samples that either are part of the listed project and still need to be processed, or for which the `secondary flag` matches:

**Note:** This script relies on internal identifiers (a tuple of `subject_id`, `sample_id` and `library_id`). Where those are missing the script tries to replace them with the equivalent external identifier, or `missing` in the case of `library_id`. This works except for top-ups or where tumor/normal pairings do not share the same internal identifiers. Please make sure older samples have the appropriate internal identifiers set in the LIMS for these use cases. 

```{r subsetSamples}
# Keep rows matching project and type requested; extract path to FASTQ
# and generate new file name for these
bcbio <- samples %>%
  mutate(project_name = if_else(project_owner == '-', 
                                project_name,
                                paste(project_owner, project_name, sep='-'))) %>%
  filter(type == TYPE &
           ((project_name == PROJECT & results == '-') | 
           (secondary_analysis == SECONDARY))) %>% 
  select(illumina_id, fastq, run, project_name, 
         subject_id, sample_id, library_id, 
         external_subject_id, external_sample_id, sample_name, 
         assay, phenotype, source, row_id) %>%
  mutate(subject_id = if_else(subject_id == '-', external_subject_id, subject_id)) %>%
  mutate(sample_id = if_else(sample_id == '-', external_sample_id, sample_id)) %>%
  mutate(library_id = if_else(library_id == '-', 'Missing', library_id)) %>%
  mutate(runname = illumina_id) %>%
<<<<<<< HEAD
  mutate(machine = str_split(illumina_id, '_')[[1]][[2]]) %>%
  select(-illumina_id)
=======
  separate(illumina_id, c(NA, 'machine', NA, NA), sep='_')
>>>>>>> 68f7615766327953a0ef22852a869f07edd266c3

# Add a 'target' name: what FASTQs are to be called after retrieval from S3
bcbio <- bcbio %>%
  mutate(targetname = paste(runname, subject_id, sample_id, library_id, sep='_')) %>%
  mutate(targetname = str_replace_all(targetname, '-', '_')) 

# Adding a sample 'description' column used to name the sample within bcbio.
# The description is now a tuple of (subject, sample, library) which should ensure topups
# are handled properly. 
bcbio <- bcbio %>%
  mutate(description = paste(subject_id, sample_id, library_id, sep='_')) %>%
  mutate(description = str_replace(description, '_topup.', '')) %>%
  mutate(description = str_replace(description, '_topup', '')) 
```


### 4. Group samples

Assigning patient/family identifiers for Peddy, assigning batches based on the subject id. Samples with the exact same name (but from different runs) are expected to be top-ups and will subsequently be merged using `bcbio_prepare_samples.py`. 

**Note:** This will likely fail for samples where we have multiple normals that are _not_ top-ups. Might have to handle these cases manually or pick one representative normal.

```{r assignBatches}
# Generating the bcbio YAML template. Keep the required columns and add information for peddy
template <- bcbio %>% 
  mutate(family_id = subject_id) %>%
  mutate(individual_id = subject_id) %>%
  select(samplename = targetname,
         description,
         batch = subject_id,
         source,
         phenotype, family_id, individual_id, row_id,
         external_subject_id,
         external_sample_id,
         external_sample_name = sample_name,
         project_name)
```

### 5. Add control information

Frequently germline-only samples are control cases for which we want to create validaton results. Assign the right benchmark set based on the subject information.

```{r assignControl}
# Define what validation data to use; all of these are shipped with bcbio so we don't have to prep standards
validations <- c('giab-NA12878', 'giab-NA24385', 'giab-NA24631')
names(validations) <- c('NA12878', 'NA24385', 'NA24631')

# Not the most elegant solution. Help appreciated to clean this up
template <- template %>% 
  mutate(validate=ifelse(external_subject_id %in% names(validations), 
                         paste(validations[external_subject_id], 'truth_small_variants.vcf.gz', sep='/'),
                         '')) %>%
  mutate(validate_regions=ifelse(external_subject_id %in% names(validations), 
                         paste(validations[external_subject_id], 'truth_regions.bed', sep='/'),
                         '')) %>%
  mutate(validate_batch=ifelse(external_subject_id %in% names(validations), 
                         external_subject_id,
                         ''))
```

### 6. Generating bcbio csv templates on a per-subject basis:

Generate a file with pointers to the sample FASTQs and their preferred new names as well as a sample descriptor (`.csv`) for bcbio to use as part of it's templating approach. The resulting directory structure can be copied to Spartan, e.g.:

> `scp -r BATCHFILES spartan:/data/cephfs/punim0010/data/Transfer/raijin/`

```{r perBatch}
write_subject <- function(complete, summary) {
  summary <- template
  complete <- 'SBJ00027'
  
  # Create a unique project name
  subject_name <- paste0(timestamp, '_', PROJECT, '_', TYPE, '_', complete)

  # Set up the project directory
  dir.create(here::here('output', subject_name, 'data'), showWarnings=FALSE, recursive=TRUE)
     
  # Subset to one individual
  summary <- summary %>%
    filter(individual_id == complete)

  # Find corresponding entries in the Google-Lims
  bcbio_subset <- bcbio %>%
     filter(row_id %in% summary$row_id)

  # Create a file list ready for linking
  #
  # With Run 81 the file structure from `bcl2fastq` changed. FASTQs are no
  # longer round in the top level run folder, but in
  # ```<Illumina_ID>/<Project>/<SampleName>/<SampleID>_R1_001.fastq.gz```
  read1 <- bcbio_subset %>%
    mutate(from = case_when(run <= 80 & machine == 'A00130' ~ 
                              paste0(fastq, '/', external_sample_id, '_R1_001.fastq.gz'),
                            run > 80 & run <= 93 & machine == 'A00130' ~ 
                              paste0(fastq, '/', project_name, '/', sample_name, '/', 
                                     external_sample_id, '_R1_001.fastq.gz'),
                            run > 93 | machine == 'A01052' ~ 
                              str_replace(fastq, '.fastq.gz', '_R1_001.fastq.gz'))) %>%
    mutate(to = paste0(targetname, '_R1_001.fastq.gz')) %>%
    select(from, to, row_id)
  
  read2 <- bcbio_subset %>%
    mutate(from = case_when(run <= 80 & machine == 'A00130' ~ 
                              paste0(fastq, '/', external_sample_id, '_R2_001.fastq.gz'),
                            run > 80 & run <= 93 & machine == 'A00130' ~ 
                              paste0(fastq, '/', project_name, '/', sample_name, '/',
                                     external_sample_id, '_R2_001.fastq.gz'),
                            run > 93 | machine == 'A01052' ~ 
                              str_replace(fastq, '.fastq.gz', '_R2_001.fastq.gz'))) %>%
    mutate(to = paste0(targetname, '_R2_001.fastq.gz')) %>%
    select(from, to, row_id)
  
  link <- rbind(read1, read2)

  # Adapt to S3 syntax which doesn't include wildcards; split path and filename for include/exclude statement
  link <- link %>%
    mutate(from_path = dirname(link$from)) %>%
    mutate(from_file = basename(link$from))

  # Write the file list for staging files. First step, `aws cp` 
  write.table(paste('aws --profile spartan_s3_readonly s3 cp', 
                    link$from_path,
                    link$row_id,
                    '--recursive --exclude "*" --include',
                    paste('"', link$from_file, '"', sep = '')),
              file = here::here('output', subject_name, 'data', paste0(subject_name, '_files.sh')),
              col.names = FALSE,
              row.names = FALSE,
              quote = FALSE)

  # Second step, rename
  write.table(paste('mv', 
                    paste(link$row_id, link$from_file, sep='/'),
                    link$to,
                    sep = ' '),
              file = here::here('output', subject_name, 'data', paste0(subject_name, '_files.sh')),
              col.names = FALSE,
              row.names = FALSE,
              quote = FALSE,
              append = TRUE)

  # Write CSV for bcbio
  summary_R1 <- summary %>% mutate(samplename = paste0(samplename, '_R1_001.fastq.gz'))
  summary_R2 <- summary %>% mutate(samplename = paste0(samplename, '_R2_001.fastq.gz'))
  summary <- rbind(summary_R1, summary_R2)

  write.table(summary,
              file = here::here('output', subject_name, 'data', paste0(subject_name, '.csv')),
              row.names = FALSE,
              col.names = TRUE,
              quote = FALSE,
              sep = ',')
}

for (subject in unique(template$batch)) {
  write_subject(subject, template)
}
```




